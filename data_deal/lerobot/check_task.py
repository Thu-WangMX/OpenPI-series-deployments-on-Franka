"""
LeRobot æ•°æ®é›†ä¿®å¤è„šæœ¬
åŠŸèƒ½ï¼š
1. æ‰‹åŠ¨è®¡ç®—å¹¶ç”Ÿæˆ stats.json (Fix Missing Stats)
2. éªŒè¯ task_index æ˜¯å¦æ­£ç¡®åŒºåˆ†äº†ä»»åŠ¡
"""

from pathlib import Path

from datasets import load_dataset
import numpy as np
import torch

# ================= é…ç½® =================
# æ•°æ®é›†æ ¹ç›®å½•
ROOT_PATH = Path("/work/wzh/huggingface/lerobot/wmx/openpi_merged_single_grasp_newest")
# =======================================


def compute_stats(dataset):
    print("ğŸ§® æ­£åœ¨è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")

    # æå–æ‰€æœ‰ action (N, 7)
    # æ³¨æ„ï¼šå¦‚æœå†…å­˜ä¸å¤Ÿï¼Œå¯ä»¥åˆ†æ‰¹è¯»å–ã€‚è¿™é‡Œ 3.6ä¸‡å¸§åº”è¯¥æ²¡é—®é¢˜ã€‚
    actions = torch.tensor(dataset["action"])

    # è®¡ç®—ç»Ÿè®¡å€¼
    stats = {
        "action": {
            "mean": actions.mean(dim=0).tolist(),
            "std": actions.std(dim=0).tolist(),
            "min": actions.min(dim=0).values.tolist(),
            "max": actions.max(dim=0).values.tolist(),
        }
    }

    # å¦‚æœæœ‰ stateï¼Œä¹Ÿè®¡ç®— state
    if "observation.state" in dataset.features:
        states = torch.tensor(dataset["observation.state"])
        stats["observation.state"] = {
            "mean": states.mean(dim=0).tolist(),
            "std": states.std(dim=0).tolist(),
            "min": states.min(dim=0).values.tolist(),
            "max": states.max(dim=0).values.tolist(),
        }

    return stats


def main():
    print(f"ğŸ“‚ ç›®æ ‡è·¯å¾„: {ROOT_PATH}")

    # 1. åŠ è½½æ•°æ®
    data_files = str(ROOT_PATH / "data/**/*.parquet")
    try:
        ds = load_dataset("parquet", data_files=data_files, split="train")
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(ds)} å¸§")
    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
        return

    # 2. éªŒè¯ Task Index åˆ†å¸ƒ
    print("\nğŸ” æ£€æŸ¥ Task Index åˆ†å¸ƒ...")
    if "task_index" in ds.features:
        task_indices = np.array(ds["task_index"])
        unique_tasks, counts = np.unique(task_indices, return_counts=True)
        print(f"   å‘ç° {len(unique_tasks)} ç§ä»»åŠ¡ ID: {unique_tasks}")
        for task_id, count in zip(unique_tasks, counts):
            print(f"   ğŸ†” Task {task_id}: {count} å¸§")

        if len(unique_tasks) >= 2:
            print("   âœ… æˆåŠŸæ£€æµ‹åˆ°æ··åˆä»»åŠ¡ (Chili vs Dolls)ï¼")
        else:
            print("   âš ï¸ è­¦å‘Š: åªå‘ç° 1 ç§ä»»åŠ¡ IDï¼Œè¯·ç¡®è®¤åˆå¹¶æ˜¯å¦æˆåŠŸã€‚")
    else:
        print("   âŒ æœªæ‰¾åˆ° task_index åˆ—ï¼")

    # 3. è®¡ç®—å¹¶ä¿å­˜ Stats
    # stats = compute_stats(ds)

    # # æ‰“å°é¢„è§ˆ
    # print("\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯é¢„è§ˆ (Action):")
    # print(f"   Mean: {np.array(stats['action']['mean'])[:3]} ...")
    # print(f"   Std:  {np.array(stats['action']['std'])[:3]} ...")

    # # æ£€æŸ¥æ˜¯å¦æœ‰éé›¶ Std (å¦‚æœå…¨æ˜¯0ï¼Œè¯´æ˜æ•°æ®æœ‰é—®é¢˜)
    # if np.all(np.array(stats['action']['std']) < 1e-6):
    #     print("   âŒ [ä¸¥é‡è­¦å‘Š] Action Std æ¥è¿‘ 0ï¼è¿™è¯´æ˜æ‰€æœ‰æ•°æ®çš„åŠ¨ä½œå¯èƒ½éƒ½æ˜¯é™æ­¢çš„ï¼Œæˆ–è€…è¯»å–é”™è¯¯ï¼")
    # else:
    #     print("   âœ… Action æ•°æ®åˆ†å¸ƒæ­£å¸¸ã€‚")

    # # 4. å†™å…¥æ–‡ä»¶
    # meta_dir = ROOT_PATH / "meta"
    # meta_dir.mkdir(exist_ok=True)
    # stats_path = meta_dir / "stats.json"

    # with open(stats_path, 'w') as f:
    #     json.dump(stats, f, indent=4)

    # print(f"\nğŸ’¾ å·²ä¿å­˜ç»Ÿè®¡ä¿¡æ¯è‡³: {stats_path}")
    # print("ğŸ‰ ä¿®å¤å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")


if __name__ == "__main__":
    main()
