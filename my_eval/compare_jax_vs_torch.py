import multiprocessing
import os
from pathlib import Path
import pickle
import random

import cv2
import numpy as np
import torch
from tqdm import tqdm

# ================= ğŸ“ é…ç½®åŒºåŸŸ =================
CONFIG_NAME = "pi0_franka_low_mem_finetune"
JAX_CHECKPOINT_DIR = "/work/wmx/openpi/checkpoints/pi0_franka_low_mem_finetune/pi0_clean_single_grasp/40000"
TORCH_CHECKPOINT_DIR = "/work/wmx/openpi/ckpt_torch/after_clean_bs32_4w"
DATA_DIR = Path("/work/wmx/openpi/data_clean/single_grasp")

# ğŸ“Š ç»Ÿè®¡è¯„ä¼°é…ç½®
EVAL_NUM_EPISODES = 20  # ç”¨å¤šå°‘ä¸ª Episode åšæ•°å€¼ç»Ÿè®¡
EVAL_FRAMES_PER_EP = 10  # æ¯ä¸ª Episode æŠ½å¤šå°‘å¸§

# ğŸ¥ è§†é¢‘å¯è§†åŒ–é…ç½®
VIS_EPISODE_IDX = 0  # æŒ‡å®šè¦æŠŠç¬¬å‡ ä¸ªæ–‡ä»¶åšæˆè§†é¢‘ (0 è¡¨ç¤ºéšæœºåˆ—è¡¨ä¸­çš„ç¬¬ä¸€ä¸ª)
OUTPUT_DIR = "vis_results"
# ===============================================


def load_pkl(path):
    with open(path, "rb") as f:
        return pickle.load(f)


def parse_frame_to_example(frame):
    """å°†ä¸€å¸§æ•°æ®è§£æä¸º OpenPI æ¨¡å‹è¾“å…¥æ ¼å¼"""
    obs = frame["observations"]
    # ä¼˜å…ˆè¯»å– language_instructionï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨é»˜è®¤
    task_desc = frame.get("language_instruction", "Put the red chili peppers into the basket")

    example = {
        "observation/image": np.array(obs["pixels"]["image"], dtype=np.uint8),
        "observation/wrist_image": np.array(obs["pixels"]["image2"], dtype=np.uint8),
        "observation/state": np.array(obs.get("observation.state", obs.get("agent_pos")), dtype=np.float32),
        "prompt": task_desc,
    }
    gt_action = np.array(frame["action"], dtype=np.float32)
    return example, gt_action


def prepare_data():
    """
    å‡†å¤‡ä¸¤ä»½æ•°æ®ï¼š
    1. eval_data: ç”¨äºè®¡ç®— MSE çš„æ•£ä¹±å¸§
    2. vis_data: ç”¨äºç”Ÿæˆè§†é¢‘çš„å®Œæ•´ Episode åºåˆ—
    """
    all_files = sorted(list(DATA_DIR.glob("*.pkl")))
    if not all_files:
        raise FileNotFoundError(f"âŒ æ²¡åœ¨ {DATA_DIR} æ‰¾åˆ° .pkl æ–‡ä»¶")

    # 1. å‡†å¤‡ç»Ÿè®¡æ•°æ® (éšæœºæŠ½æ ·)
    eval_files = random.sample(all_files, min(len(all_files), EVAL_NUM_EPISODES))
    eval_batch = []

    print(f"ğŸ“Š æ­£åœ¨åŠ è½½ç»Ÿè®¡æ•°æ® ({len(eval_files)} episodes)...")
    for pkl_path in eval_files:
        data = load_pkl(pkl_path)
        indices = random.sample(range(len(data)), min(len(data), EVAL_FRAMES_PER_EP))
        for idx in indices:
            ex, gt = parse_frame_to_example(data[idx])
            eval_batch.append(({"id": f"{pkl_path.name}_{idx}"}, ex, gt))

    # 2. å‡†å¤‡è§†é¢‘æ•°æ® (å–ä¸€ä¸ªå®Œæ•´æ–‡ä»¶)
    vis_file = all_files[VIS_EPISODE_IDX % len(all_files)]
    print(f"ğŸ¥ æ­£åœ¨åŠ è½½å¯è§†åŒ–æ•°æ® (å®Œæ•´ Episode): {vis_file.name} ...")
    vis_data_raw = load_pkl(vis_file)
    vis_batch = []
    for idx, frame in enumerate(vis_data_raw):
        ex, gt = parse_frame_to_example(frame)
        vis_batch.append(({"id": f"vis_{idx}"}, ex, gt))

    return eval_batch, vis_batch, vis_file.name


# ==============================================================================
#  æ¨ç†è¿›ç¨‹ (Multiprocessing Worker)
# ==============================================================================
def _worker_process(config_name, ckpt_dir, eval_data, vis_data, backend, queue):
    try:
        print(f"\nğŸš€ [{backend}] è¿›ç¨‹å¯åŠ¨...")
        if backend == "JAX":
            os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"] = "false"
            os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"] = "platform"

        from openpi.policies import policy_config
        from openpi.training import config as _config

        config = _config.get_config(config_name)
        # åŠ è½½ Policy
        policy = policy_config.create_trained_policy(config, ckpt_dir)

        # å®šä¹‰æ¨ç†å‡½æ•°
        def infer_list(data_list, desc):
            res = []
            for _, example, _ in tqdm(data_list, desc=desc):
                if backend == "Torch":
                    with torch.inference_mode():
                        out = policy.infer(example)
                        action = out["actions"][0]
                        if isinstance(action, torch.Tensor):
                            action = action.cpu().numpy()
                else:
                    out = policy.infer(example)
                    action = np.array(out["actions"])[0]
                res.append(action)
            return res

        # 1. è·‘ç»Ÿè®¡æ•°æ®
        eval_res = infer_list(eval_data, f"{backend} Eval")
        # 2. è·‘è§†é¢‘æ•°æ®
        vis_res = infer_list(vis_data, f"{backend} Vis")

        queue.put((True, eval_res, vis_res))

    except Exception as e:
        import traceback

        traceback.print_exc()
        queue.put((False, str(e), None))


def run_inference(backend, ckpt_dir, eval_data, vis_data):
    ctx = multiprocessing.get_context("spawn")
    queue = ctx.Queue()
    p = ctx.Process(target=_worker_process, args=(CONFIG_NAME, ckpt_dir, eval_data, vis_data, backend, queue))
    p.start()

    try:
        success, eval_res, vis_res = queue.get()
    except Exception as e:
        p.terminate()
        raise RuntimeError(f"{backend} æ•°æ®è·å–å¤±è´¥: {e}")

    p.join()
    if not success:
        raise RuntimeError(f"{backend} è¿è¡ŒæŠ¥é”™: {eval_res}")
    return eval_res, vis_res


# ==============================================================================
#  å¯è§†åŒ–ç»˜åˆ¶å·¥å…·
# ==============================================================================
def draw_text_with_bg(img, text, pos, font_scale=0.4, text_color=(255, 255, 255), bg_color=(0, 0, 0)):
    font = cv2.FONT_HERSHEY_SIMPLEX
    thickness = 1
    x, y = pos
    (w, h), baseline = cv2.getTextSize(text, font, font_scale, thickness)
    cv2.rectangle(img, (x, y - h - 4), (x + w, y + baseline), bg_color, -1)
    cv2.putText(img, text, (x, y), font, font_scale, text_color, thickness, cv2.LINE_AA)


def generate_video(vis_batch, jax_res, torch_res, filename):
    print(f"\nğŸ¬ æ­£åœ¨ç”Ÿæˆå¯¹æ¯”è§†é¢‘: {filename}")
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    save_path = os.path.join(OUTPUT_DIR, filename)

    # è·å–ç¬¬ä¸€å¸§å°ºå¯¸
    img1 = vis_batch[0][1]["observation/image"]
    img2 = vis_batch[0][1]["observation/wrist_image"]
    h, w, _ = img1.shape

    # ç”»å¸ƒå¸ƒå±€: ä¸Šæ–¹ç•™ 120px å†™å­—ï¼Œä¸‹æ–¹å·¦å³æ‹¼æ¥å›¾åƒ
    header_h = 130
    canvas_w = w * 2
    canvas_h = h + header_h

    fourcc = cv2.VideoWriter_fourcc(*"mp4v")
    out = cv2.VideoWriter(save_path, fourcc, 10, (canvas_w, canvas_h))

    for i in tqdm(range(len(vis_batch)), desc="Rendering"):
        _, ex, gt = vis_batch[i]
        j_act = jax_res[i]
        t_act = torch_res[i]

        # 1. å›¾åƒéƒ¨åˆ†
        im1_bgr = cv2.cvtColor(ex["observation/image"], cv2.COLOR_RGB2BGR)
        im2_bgr = cv2.cvtColor(ex["observation/wrist_image"], cv2.COLOR_RGB2BGR)
        imgs_combined = np.hstack([im1_bgr, im2_bgr])

        # 2. èƒŒæ™¯éƒ¨åˆ†
        canvas = np.zeros((canvas_h, canvas_w, 3), dtype=np.uint8)
        canvas[header_h:, :, :] = imgs_combined

        # 3. ç»˜åˆ¶æ–‡å­— (ä¸‰è¡Œ Action)
        # æ ¼å¼åŒ–å‡½æ•°
        def fmt_act(name, act, color):
            # åˆ†æˆä¸¤æ®µæ˜¾ç¤º: J1-4 | J5-G
            s1 = ", ".join([f"{x:6.3f}" for x in act[:4]])
            s2 = ", ".join([f"{x:6.3f}" for x in act[4:]])
            return f"{name}: [{s1} | {s2}]"

        # ç»˜åˆ¶
        draw_text_with_bg(canvas, f"Frame: {i:03d} | Task: {ex['prompt']}", (10, 20), font_scale=0.5)

        # GT (Green)
        draw_text_with_bg(canvas, fmt_act("GT   ", gt, None), (10, 50), text_color=(0, 255, 0))
        # JAX (Cyan)
        draw_text_with_bg(canvas, fmt_act("JAX  ", j_act, None), (10, 80), text_color=(255, 255, 0))
        # Torch (Orange/Blue in BGR)
        draw_text_with_bg(canvas, fmt_act("TORCH", t_act, None), (10, 110), text_color=(0, 165, 255))

        out.write(canvas)

    out.release()
    print(f"âœ… è§†é¢‘å·²ä¿å­˜: {save_path}")


# ==============================================================================
#  ç»Ÿè®¡è®¡ç®—
# ==============================================================================
def compute_stats(eval_batch, jax_res, torch_res):
    print(f"\nğŸ“Š è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡ (å…± {len(eval_batch)} å¸§)...")

    jax_mses = []
    torch_mses = []

    for i in range(len(eval_batch)):
        gt = eval_batch[i][2]
        j_a = jax_res[i]
        t_a = torch_res[i]

        # ç®€å•çš„ MSE
        jax_mses.append(np.mean((j_a - gt) ** 2))
        torch_mses.append(np.mean((t_a - gt) ** 2))

    avg_j = np.mean(jax_mses)
    avg_t = np.mean(torch_mses)

    print("-" * 40)
    print(f"JAX Mean MSE   : {avg_j:.6f}")
    print(f"Torch Mean MSE : {avg_t:.6f}")
    print(f"Diff (J - T)   : {avg_j - avg_t:.6f}")
    print("-" * 40)

    if abs(avg_j - avg_t) < 1e-5:
        print("âœ… ä¸¤ä¸ªæ¡†æ¶æ¨ç†ç»“æœåŸºæœ¬ä¸€è‡´")
    else:
        print("âš ï¸ å­˜åœ¨ç²¾åº¦å·®å¼‚ï¼Œè¯·æ£€æŸ¥")


# ==============================================================================
#  ä¸»å‡½æ•°
# ==============================================================================
def main():
    # 1. å‡†å¤‡æ•°æ®
    eval_data, vis_data, vis_filename = prepare_data()

    # 2. JAX æ¨ç†
    print("\n>>> å¼€å§‹ JAX æ¨ç†...")
    j_eval, j_vis = run_inference("JAX", JAX_CHECKPOINT_DIR, eval_data, vis_data)

    # 3. Torch æ¨ç†
    print("\n>>> å¼€å§‹ Torch æ¨ç†...")
    t_eval, t_vis = run_inference("Torch", TORCH_CHECKPOINT_DIR, eval_data, vis_data)

    # 4. ç”Ÿæˆè§†é¢‘
    video_name = f"compare_{vis_filename.replace('.pkl', '')}.mp4"
    generate_video(vis_data, j_vis, t_vis, video_name)

    # 5. è¾“å‡ºç»Ÿè®¡
    compute_stats(eval_data, j_eval, t_eval)


if __name__ == "__main__":
    try:
        multiprocessing.set_start_method("spawn", force=True)
    except RuntimeError:
        pass
    main()
